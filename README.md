# Decision Trees vs. Ensembles in Regressionâ€‘Based Offline RL

A practical and reproducible framework for the experiments described in â€œDecision Trees vs. Ensembles in Regressionâ€‘Based Offline RLâ€ (Bachelor thesis, TU Delft, 2025). This repository collects code, configs and helper scripts to train decisionâ€‘tree and gradientâ€‘boosted baselines on the [D4RL](https://github.com/rail-berkeley/d4rl) benchmarks.

---

## âœ¨ Features

* **Single commandâ€line entryâ€‘point** â€“ `python -m main` wraps training, evaluation, result collection and plotting.
* **Declarative experiment specs** â€“ YAML (or Python) configs expand into full factorial grids of *model Ã— env Ã— seed Ã— RTG strategy*.
* **Reproducible storage layout** â€“ models and scores live under a predictable `models/{MODEL_CODE}/{ENV}/seedâ€‘{N}/{RTG_SLUG}` tree.
* **Zeroâ€‘boilerplate plotting** â€“ builtâ€‘in scripts generate the paper figures (families, RTG sweeps, capacity tiers, â€¦).
* **Selfâ€‘contained model codes** â€“ strings like `CART-D2L16-0-10-0` make huge hyperâ€‘parameter grids reproducible without extra bookkeeping.

---

## ğŸ“¦ Installation

```bash
# Clone
$ git clone https://github.com/rae-ra/Decision-Trees-vs.-Ensembles-in-Regression-Based-Offline-RL
$ cd Decision-Trees-vs.-Ensembles-in-Regression-Based-Offline-RL

# Create env from template
$ conda env create -f environment.yml
$ conda activate offline_rl_shallow_dt
```
---

## ğŸ—‚ Repository layout

```
â”œâ”€â”€ main.py                     # Unified CLI (run/collect/plot/delete/aggregate)
â”œâ”€â”€ configs/                    # Experiment YAMLs (examples below)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                 # Model wrappers (CART, OptimalTrees, XGB â€¦)
â”‚   â”œâ”€â”€ runners/                # Command implementations used by main.py
â”‚   â””â”€â”€ utils/                  # Data, FSâ€‘layout, YAML loader, code enc/dec â€¦
â”œâ”€â”€ models/                     # Generated artefacts (ignored by Git)
â”œâ”€â”€ plots/                      # Generated artefacts (ignored by Git)
â””â”€â”€ results/                    # Generated artefacts (ignored by Git)
```

A full description of the *generated* folder structure lives in `src/utils/fs_layout.py` but the gist is:

```
models/
  CART-D0L32-0-0-1/
    HC-M/
      seed-0/
        max/score.json
        fixed-1p00/score.json
        tree.pkl  hyperparams.json
      env_best.json   # summarised over seeds
```

---

## âš™ï¸  Writing experiment YAMLs

Below is a minimal yet complete example covering all three supported task formats (flat, code shorthand, and grid). Save it as `configs/example.yml` and run `python -m main run configs/example.yml`.

```yaml
tasks:
  # 1) Flat definition ---------------------------------------------------
  - type: m-cart
    hyperparams: {max_depth: 6, max_leaf_nodes: 64, scale_obs: true}
    envs: [HC-M, HO-M]
    seeds: [0, 1]
    rtgs:
      - {name: max}
      - {name: fixed, fixed_norm: 1.0}
    save_model: true

  # 2) Code shorthand ----------------------------------------------------
  - code: M-CART-D6L64-1-10-2   # encodes type + hyperparams
    envs: [HC-M]
    seeds: [0]
    rtgs: [{name: fixed, fixed_norm: 1.0}]

  # 3) Grid expansion ----------------------------------------------------
  - grid:
      models:
        - code: CART-D0L32-0-0-1
        - type: xgb
          hyperparams: {n_estimators: 1024, scale_obs: false}
      envs: [W-M, W-ME]
      seeds: [0, 42]
      rtgs: [{name: max}]
```

*YAML tips*

* Use the short env codes (`HC-M`) or the full names (`halfcheetah-medium`) interchangeably.
* Grids expand to **all** combinations, so the above spawns 2 Ã— 2 Ã— 2 Ã— 1 = 8 tasks automatically.

---

## ğŸ— Model codes & runtime flags

### Model code anatomy

```
CART-D{depth}L{leafs}-{scale}-{min_leaf}-{alpha_idx}
M-CART-D{depth}L{leafs}-{scale}-{min_leaf}-{alpha_idx}
OPTI-D{depth}L{leafs}-{min_leaf}
XGB-{scale}-{n_estimators}
```

| Token             | Meaning                                                       |
| ----------------- | ------------------------------------------------------------- |
| **CART / Mâ€‘CART** | Singleâ€‘ vs multiâ€‘output decision tree                         |
| **D**             | Max depth (0 â†’ unlimited)                                     |
| **L**             | Max leaf nodes (0 â†’ unlimited)                                |
| **scale**         | `1` = apply `StandardScaler` to observations, `0` = raw       |
| **min\_leaf**     | Minimum samples per leaf                                      |
| **alpha\_idx**    | Costâ€‘complexity pruning Î± index (0 â†’ 0.0, 1 â†’ 1eâ€‘3, 2 â†’ 1eâ€‘2) |
| **n\_estimators** | Number of boosting rounds for XGBoost                         |

### TaskSpec runtime flags

| Flag             | Default | Purpose                                                             |
| ---------------- | ------- | ------------------------------------------------------------------- |
| `save_model`     | `false` | Persist the trained estimator under `models/â€¦` for later inspection |
| `load_model`     | `false` | Skip training and load an existing model if the files are present   |
| `skip_if_result` | `true`  | Donâ€™t reâ€‘evaluate if a valid `score.json` already exists            |

These options let you checkpoint heavy sweeps, resume interrupted runs, or regenerate plots without touching any source code.

---

## ğŸš€ Running the pipeline

| Stage                | Command                                             | Description                                                  |
| -------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| **Train + Eval**     | `python -m main run configs/halfcheetah_medium.yml` | Train each model and roll out *N* episodes per RTG strategy. |
| **Collect**          | `python -m main collect`                            | Aggregate raw `score.json` files into tidy CSV summaries.    |
| **Plot**             | `python -m main plot --task families`               | Produce paper figures (saved under `plots/`).                |
| **Clean**            | `python -m main delete --pattern CART-*`            | Delete artefacts that match a glob under `models/`.          |
| **Aggregate tables** | `python -m main aggregate`                          | Output crossâ€‘task CSV / LaTeX tables.                        |

---

## ğŸ–¼  Plotting tasks

* `families` â€“ Figure 2: score vs. environment grouped by model family.
* `rtg` â€“ Figure 3: RTG sweeps (needs `--models` and `--env` args).
* `models` â€“ Boxâ€‘plots comparing multiple model codes for one env.
* `complexity_sweep` â€“ Depth/leaf sweeps for CARTâ€style trees.
* `capacity_tiers` â€“ Paper appendix: capacity tiers breakdown.

All figures are dropped into the `plots/` folder unless `--out` is provided.

---

## ğŸ“Š Reproducing paper numbers

1. Install the env `(conda env create -f environment.yml)`.
2. Download the D4RL datasets once (Gym will cache them).
3. For every result reported in the paper, write its model code(s), hyper-parameters, seeds, RTG strategies, and target environments into a YAML file that follows the schema in *Writing experiment YAMLs*.
4. `python -m main run path/to/your_experiments.yml` followed by `python -m main collect` will generate a `results/<MODEL_CODE>.csv` with the results.

---

## License

GNU General Public License v3.0 â€“ see `LICENSE` for details.
